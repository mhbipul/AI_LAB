{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNY0jsCLoM7TxDYkLsFrSm5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"CJRcU8JHKmb8","executionInfo":{"status":"error","timestamp":1681016985913,"user_tz":-360,"elapsed":596,"user":{"displayName":"Mahmudul Hasan Bipul","userId":"08456904650511656038"}},"outputId":"c2680562-5e1a-4ea7-c274-3cf1a306f5fa"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-94613bf20bbb>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Load and split the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'iris.data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_iris_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-94613bf20bbb>\u001b[0m in \u001b[0;36mload_iris_dataset\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load iris dataset from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_iris_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iris.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'iris'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid mode: 'iris.csv'"]}],"source":["import numpy as np\n","from numpy import genfromtxt\n","\n","# Load iris dataset from file\n","def load_iris_dataset(filename):\n","    with open(filename, 'iris.csv') as f:\n","        lines = f.readlines()\n","        data_path = 'iris'\n","        for line in lines:\n","            values = line.strip().split(',')\n","            sample = [float(value) for value in values[:-1]] + [values[-1]]\n","            data.append(sample)\n","        return data\n","\n","# Split the dataset into training, validation, and test set\n","def split_dataset(dataset):\n","    train_set, val_set, test_set = [], [], []\n","    for sample in dataset:\n","        r = np.random.rand()\n","        if r >= 0 and r <= 0.7:\n","            train_set.append(sample)\n","        elif r > 0.7 and r <= 0.85:\n","            val_set.append(sample)\n","        else:\n","            test_set.append(sample)\n","    return train_set, val_set, test_set\n","\n","# Define K value\n","K = 5\n","\n","# Define function to calculate Euclidean distance between two samples\n","def euclidean_distance(sample1, sample2):\n","    return np.sqrt(np.sum((sample1[:-1] - sample2[:-1])**2))\n","\n","# Define function to perform KNN classification\n","def knn_classification(train_set, val_set, K):\n","    correct = 0\n","    for val_sample in val_set:\n","        distances = []\n","        for train_sample in train_set:\n","            distance = euclidean_distance(val_sample, train_sample)\n","            distances.append((train_sample, distance))\n","        distances.sort(key=lambda x: x[1])\n","        nearest_neighbors = [d[0] for d in distances[:K]]\n","        classes = [neighbor[-1] for neighbor in nearest_neighbors]\n","        predicted_class = max(set(classes), key=classes.count)\n","        if predicted_class == val_sample[-1]:\n","            correct += 1\n","    accuracy = correct / len(val_set) * 100\n","    return accuracy\n","\n","# Load and split the dataset\n","filename = 'iris.data'\n","dataset = load_iris_dataset(filename)\n","train_set, val_set, test_set = split_dataset(dataset)\n","\n","# Perform KNN classification on the validation set\n","validation_accuracy = knn_classification(train_set, val_set, K)\n","\n","# Print the validation accuracy\n","print(f\"Validation accuracy: {validation_accuracy}%\")\n"]}]}